{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8710c07d",
   "metadata": {},
   "source": [
    "# Big Data Mysql and Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2f23e",
   "metadata": {},
   "source": [
    "## Start session Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052f6cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90833baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/19 14:25:44 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "#start spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Pyspark to MySQL\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54761e",
   "metadata": {},
   "source": [
    "## Loading the data from Hadoop using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d1c19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "| ID|   unknown|               date|    flag|           user|                text|\n",
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|2009-04-07 06:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|2009-04-07 06:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-07 06:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|2009-04-07 06:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|2009-04-07 06:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|2009-04-07 06:20:00|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|2009-04-07 06:20:03|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|2009-04-07 06:20:03|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|2009-04-07 06:20:05|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|2009-04-07 06:20:09|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|2009-04-07 06:20:16|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|2009-04-07 06:20:17|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|2009-04-07 06:20:19|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|2009-04-07 06:20:19|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|2009-04-07 06:20:20|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|2009-04-07 06:20:20|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|2009-04-07 06:20:22|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|2009-04-07 06:20:25|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|2009-04-07 06:20:31|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|2009-04-07 06:20:34|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define schema for the DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", LongType(), True),\n",
    "    StructField(\"unknown\", StringType(), True),\n",
    "    StructField(\"date\", TimestampType(), True),\n",
    "    StructField(\"flag\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read the data with the correct timestampFormat fron hadoop\n",
    "tweets_df = spark.read.csv(\"hdfs:///CA2/ProjectTweets.csv\", header=False, schema=schema, sep=\",\", timestampFormat=\"EEE MMM dd HH:mm:ss zzz yyyy\")\n",
    "\n",
    "# Show the data\n",
    "tweets_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6ac78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- unknown: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- flag: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check Schema to check data types and structure of the data\n",
    "tweets_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae22737",
   "metadata": {},
   "source": [
    "# Read from MySQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b28a34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "| ID|   unknown|               date|    flag|           user|                text|\n",
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|2009-04-07 06:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|2009-04-07 06:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-07 06:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|2009-04-07 06:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|2009-04-07 06:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|2009-04-07 06:20:00|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|2009-04-07 06:20:03|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|2009-04-07 06:20:03|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|2009-04-07 06:20:05|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|2009-04-07 06:20:09|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|2009-04-07 06:20:16|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|2009-04-07 06:20:17|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|2009-04-07 06:20:19|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|2009-04-07 06:20:19|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|2009-04-07 06:20:20|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|2009-04-07 06:20:20|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|2009-04-07 06:20:22|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|2009-04-07 06:20:25|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|2009-04-07 06:20:31|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|2009-04-07 06:20:34|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Let's check if we can read the mysql database\n",
    "\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read from MySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define database connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/Tweets\"\n",
    "properties = {\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "# Define the table you want to read\n",
    "table = \"tweet_data\"\n",
    "\n",
    "# Read data from the SQL database into a DataFrame\n",
    "tweets_sql_df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "\n",
    "# Show the DataFrame\n",
    "tweets_sql_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7460642d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/05/19 14:26:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to MySQL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Check is data is written in Mysql\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Write to MySQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Replace the connection parameters with your MySQL connection details\n",
    "username = 'root'\n",
    "password = 'password'\n",
    "host = 'localhost'\n",
    "database_name = 'Tweets'\n",
    "table_name = 'tweet_data'\n",
    "\n",
    "# Convert Spark DataFrame to temporary view\n",
    "tweets_df.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "# Define JDBC URL\n",
    "jdbc_url = f\"jdbc:mysql://{host}/{database_name}\"\n",
    "\n",
    "# Define properties for the JDBC connection\n",
    "connection_properties = {\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Write DataFrame to MySQL table\n",
    "tweets_df.write.jdbc(url=jdbc_url, table=table_name, mode=\"overwrite\", properties=connection_properties)\n",
    "\n",
    "print(\"Data successfully written to MySQL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8d440ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "| ID|   unknown|               date|    flag|           user|                text|\n",
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "|  0|1467810369|2009-04-07 06:19:45|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|2009-04-07 06:19:49|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-07 06:19:53|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|2009-04-07 06:19:57|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|2009-04-07 06:19:57|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|2009-04-07 06:20:00|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|2009-04-07 06:20:03|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|2009-04-07 06:20:03|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|2009-04-07 06:20:05|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|2009-04-07 06:20:09|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|2009-04-07 06:20:16|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|2009-04-07 06:20:17|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|2009-04-07 06:20:19|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|2009-04-07 06:20:19|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|2009-04-07 06:20:20|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|2009-04-07 06:20:20|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|2009-04-07 06:20:22|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|2009-04-07 06:20:25|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|2009-04-07 06:20:31|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|2009-04-07 06:20:34|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+-------------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Let's check if the data is filled in the Mysql\n",
    "tweets_sql_df = spark.read.jdbc(url=url, table=table, properties=properties)\n",
    "\n",
    "# Show the DataFrame\n",
    "tweets_sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043aac93",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75933858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+--------+--------------------+--------------------+\n",
      "|summary|               ID|             unknown|    flag|                user|                text|\n",
      "+-------+-----------------+--------------------+--------+--------------------+--------------------+\n",
      "|  count|          1600000|             1600000| 1600000|             1600000|             1600000|\n",
      "|   mean|         799999.5|1.9988175522956276E9|    null| 4.325887521835714E9|                null|\n",
      "| stddev|461880.3596892604|1.9357607362268892E8|    null|5.162733218454885E10|                null|\n",
      "|    min|                0|          1467810369|NO_QUERY|        000catnap000|                 ...|\n",
      "|    max|          1599999|          2329205794|NO_QUERY|          zzzzeus111|ï¿½ï¿½ï¿½ï¿½ï¿½ß§...|\n",
      "+-------+-----------------+--------------------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Check Statistic of dataset\n",
    "tweets_sql_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ee55a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing dates: 39\n",
      "Actual missing dates:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2009-04-08|\n",
      "|2009-04-09|\n",
      "|2009-04-10|\n",
      "|2009-04-11|\n",
      "|2009-04-12|\n",
      "|2009-04-13|\n",
      "|2009-04-14|\n",
      "|2009-04-15|\n",
      "|2009-04-16|\n",
      "|2009-04-17|\n",
      "|2009-04-22|\n",
      "|2009-04-23|\n",
      "|2009-04-24|\n",
      "|2009-04-25|\n",
      "|2009-04-26|\n",
      "|2009-04-27|\n",
      "|2009-04-28|\n",
      "|2009-04-29|\n",
      "|2009-04-30|\n",
      "|2009-05-01|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, min, max\n",
    "from pyspark.sql.types import DateType\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "# Convert 'date' column to DateType \n",
    "tweets_sql_df = tweets_sql_df.withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# Find min and max dates\n",
    "min_date = tweets_sql_df.selectExpr(\"min(date)\").first()[0]\n",
    "max_date = tweets_sql_df.selectExpr(\"max(date)\").first()[0]\n",
    "\n",
    "# Generate DataFrame with all dates within the range\n",
    "date_range = [min_date + timedelta(days=x) for x in range((max_date - min_date).days + 1)]\n",
    "date_tweets = spark.createDataFrame([(date,) for date in date_range], [\"date\"]).withColumn(\"date\", col(\"date\").cast(DateType()))\n",
    "\n",
    "# Left join with the original DataFrame to find missing dates\n",
    "missing_dates_df = date_tweets.join(tweets_sql_df, \"date\", \"left_anti\")\n",
    "\n",
    "missing_dates_count = missing_dates_df.count()\n",
    "\n",
    "if missing_dates_count > 0:\n",
    "    print(\"Number of missing dates:\", missing_dates_count)\n",
    "    print(\"Actual missing dates:\")\n",
    "    missing_dates_df.show()\n",
    "else:\n",
    "    print(\"No missing dates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2a327ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows based on user, date, and text:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+-----+\n",
      "|         user|      date|                text|count|\n",
      "+-------------+----------+--------------------+-----+\n",
      "|       LouluS|2009-04-19|reading the islan...|    2|\n",
      "|     tweetpet|2009-04-20|@astrid35  Clean Me!|    2|\n",
      "|     eBlondie|2009-05-04|it's raining here...|    2|\n",
      "|  elenaaaaaaa|2009-05-10|i'm dreading tomo...|    2|\n",
      "|       Tayzor|2009-05-11|Heading back to L...|    2|\n",
      "|  guiltfeeder|2009-05-17|@Keinessish Yeah ...|    2|\n",
      "|XxSuperHansxX|2009-05-22|It's so warm.... ...|    2|\n",
      "|  cbruton1975|2009-06-01|Found out last ni...|    2|\n",
      "|Jeff_Hardyfan|2009-05-29|&quot;@mileycyrus...|    2|\n",
      "|   HeyMoonday|2009-06-06|You tried your be...|    2|\n",
      "|    belgarcia|2009-06-22|It's one of the l...|    2|\n",
      "|    MsStaceyK|2009-06-16|   @muSicFienDkiCks |    2|\n",
      "|     tweetpet|2009-04-21|@MidasJackson  Cl...|    2|\n",
      "| raeraeverret|2009-05-03|Shreveport this w...|    2|\n",
      "|      _Unica_|2009-05-04|Sweeeet Sunday..n...|    2|\n",
      "|     diiilxia|2009-05-04|@DavidArchie watc...|    2|\n",
      "|   CarissaWee|2009-05-04|I am totally envi...|    3|\n",
      "|      bitihal|2009-05-10|@JonathanRKnight ...|    2|\n",
      "|  lalaATROPHY|2009-05-17|w00t I got a 94  ...|    2|\n",
      "|    olivia_15|2009-05-18|Dinner Was Good  ...|    2|\n",
      "+-------------+----------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Check for duplicates based on user, date, and text\n",
    "duplicates = tweets_sql_df.groupBy(\"user\", \"date\", \"text\").count().where(col(\"count\") > 1)\n",
    "\n",
    "if duplicates.count() > 0:\n",
    "    print(\"Duplicate rows based on user, date, and text:\")\n",
    "    duplicates.show()\n",
    "else:\n",
    "    print(\"No duplicates found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10133dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date| count|\n",
      "+----------+------+\n",
      "|2009-04-07| 20671|\n",
      "|2009-04-18| 16132|\n",
      "|2009-04-19| 33670|\n",
      "|2009-04-20| 18447|\n",
      "|2009-04-21| 11105|\n",
      "|2009-05-02| 31096|\n",
      "|2009-05-03| 25045|\n",
      "|2009-05-04| 29823|\n",
      "|2009-05-10| 31551|\n",
      "|2009-05-11|  6217|\n",
      "|2009-05-12|  4186|\n",
      "|2009-05-14| 21526|\n",
      "|2009-05-17| 41205|\n",
      "|2009-05-18| 44564|\n",
      "|2009-05-22| 41206|\n",
      "|2009-05-24|   169|\n",
      "|2009-05-25|   169|\n",
      "|2009-05-27| 11619|\n",
      "|2009-05-29| 55874|\n",
      "|2009-05-30|103990|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check the distribution of tweet timestamps\n",
    "tweets_sql_df.groupBy(\"date\").count().orderBy(\"date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aaf2228",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      date| count|\n",
      "+----------+------+\n",
      "|2009-06-07|112221|\n",
      "|2009-05-31|105820|\n",
      "|2009-06-06|104053|\n",
      "|2009-05-30|103990|\n",
      "|2009-06-01| 95479|\n",
      "|2009-06-16| 90410|\n",
      "|2009-06-02| 81633|\n",
      "|2009-06-15| 78395|\n",
      "|2009-06-03| 61265|\n",
      "|2009-05-29| 55874|\n",
      "+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "# Group by date and count the number of tweets for each date\n",
    "tweets_by_date = tweets_sql_df.groupBy(\"date\").count()\n",
    "\n",
    "# Order by count in descending order\n",
    "tweets_by_date_sorted = tweets_by_date.orderBy(desc(\"count\"))\n",
    "\n",
    "# Show the top 10 days with the most tweets\n",
    "top_10_days = tweets_by_date_sorted.limit(10)\n",
    "top_10_days.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68bebdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|           user|count|\n",
      "+---------------+-----+\n",
      "|       lost_dog|  549|\n",
      "|        webwoke|  345|\n",
      "|       tweetpet|  310|\n",
      "|SallytheShizzle|  281|\n",
      "|    VioletsCRUK|  279|\n",
      "|    mcraddictal|  276|\n",
      "|       tsarnick|  248|\n",
      "|    what_bugs_u|  246|\n",
      "|    Karen230683|  238|\n",
      "|      DarkPiano|  236|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find top users who tweeted the most\n",
    "top_users = tweets_sql_df.groupBy(\"user\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the top 10 users with the most tweets\n",
    "top_10_users = top_users.limit(10)\n",
    "top_10_users.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00fcd16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|word|  count|\n",
      "+----+-------+\n",
      "|    |1930617|\n",
      "|  to| 552962|\n",
      "|   I| 496608|\n",
      "| the| 487500|\n",
      "|   a| 366212|\n",
      "|  my| 280025|\n",
      "| and| 275263|\n",
      "|   i| 249975|\n",
      "|  is| 217692|\n",
      "| you| 213871|\n",
      "+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "# Perform frequency analysis in tweet text\n",
    "word_counts = tweets_sql_df.select(explode(split(\"text\", \" \")).alias(\"word\")) \\\n",
    "                      .groupBy(\"word\").count() \\\n",
    "                      .orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Show the top 10 most frequent words\n",
    "top_10_words = word_counts.limit(10)\n",
    "top_10_words.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a676509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 52:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+--------+---------------+--------------------+\n",
      "| ID|   unknown|      date|    flag|           user|                text|\n",
      "+---+----------+----------+--------+---------------+--------------------+\n",
      "|  0|1467810369|2009-04-07|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|1467810672|2009-04-07|NO_QUERY|  scotthamilton|is upset that he ...|\n",
      "|  2|1467810917|2009-04-07|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
      "|  3|1467811184|2009-04-07|NO_QUERY|        ElleCTF|my whole body fee...|\n",
      "|  4|1467811193|2009-04-07|NO_QUERY|         Karoli|@nationwideclass ...|\n",
      "|  5|1467811372|2009-04-07|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|1467811592|2009-04-07|NO_QUERY|        mybirch|         Need a hug |\n",
      "|  7|1467811594|2009-04-07|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|1467811795|2009-04-07|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|1467812025|2009-04-07|NO_QUERY|        mimismo|@twittera que me ...|\n",
      "| 10|1467812416|2009-04-07|NO_QUERY| erinx3leannexo|spring break in p...|\n",
      "| 11|1467812579|2009-04-07|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
      "| 12|1467812723|2009-04-07|NO_QUERY|           TLeC|@caregiving I cou...|\n",
      "| 13|1467812771|2009-04-07|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|1467812784|2009-04-07|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
      "| 15|1467812799|2009-04-07|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|1467812964|2009-04-07|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
      "| 17|1467813137|2009-04-07|NO_QUERY|       armotley|about to file taxes |\n",
      "| 18|1467813579|2009-04-07|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|1467813782|2009-04-07|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+----------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd32ce",
   "metadata": {},
   "source": [
    "## Dropping unnecesary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1f07385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 53:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------+--------------------+\n",
      "| ID|      date|           user|                text|\n",
      "+---+----------+---------------+--------------------+\n",
      "|  0|2009-04-07|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|2009-04-07|  scotthamilton|is upset that he ...|\n",
      "|  2|2009-04-07|       mattycus|@Kenichan I dived...|\n",
      "|  3|2009-04-07|        ElleCTF|my whole body fee...|\n",
      "|  4|2009-04-07|         Karoli|@nationwideclass ...|\n",
      "|  5|2009-04-07|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|2009-04-07|        mybirch|         Need a hug |\n",
      "|  7|2009-04-07|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|2009-04-07|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|2009-04-07|        mimismo|@twittera que me ...|\n",
      "| 10|2009-04-07| erinx3leannexo|spring break in p...|\n",
      "| 11|2009-04-07|   pardonlauren|I just re-pierced...|\n",
      "| 12|2009-04-07|           TLeC|@caregiving I cou...|\n",
      "| 13|2009-04-07|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|2009-04-07|    bayofwolves|@smarrison i woul...|\n",
      "| 15|2009-04-07|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|2009-04-07| lovesongwriter|Hollis' death sce...|\n",
      "| 17|2009-04-07|       armotley|about to file taxes |\n",
      "| 18|2009-04-07|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|2009-04-07|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Let's delete the unknown column\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Drop the 'unknown' and 'flag' columns\n",
    "tweets_sql_df= tweets_sql_df.drop('unknown', 'flag')\n",
    "\n",
    "# Show the updated DataFrame\n",
    "tweets_sql_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33105311",
   "metadata": {},
   "source": [
    "# Overwrite the data in MySQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "037acc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Overwrite the table in MySQL with the updated DataFrame\n",
    "tweets_sql_df.write.jdbc(url=jdbc_url, table=\"tweets_mysql_processed\", mode=\"overwrite\", properties=connection_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae2d286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------------+--------------------+\n",
      "| ID|      date|           user|                text|\n",
      "+---+----------+---------------+--------------------+\n",
      "|  0|2009-04-07|_TheSpecialOne_|@switchfoot http:...|\n",
      "|  1|2009-04-07|  scotthamilton|is upset that he ...|\n",
      "|  2|2009-04-07|       mattycus|@Kenichan I dived...|\n",
      "|  3|2009-04-07|        ElleCTF|my whole body fee...|\n",
      "|  4|2009-04-07|         Karoli|@nationwideclass ...|\n",
      "|  5|2009-04-07|       joy_wolf|@Kwesidei not the...|\n",
      "|  6|2009-04-07|        mybirch|         Need a hug |\n",
      "|  7|2009-04-07|           coZZ|@LOLTrish hey  lo...|\n",
      "|  8|2009-04-07|2Hood4Hollywood|@Tatiana_K nope t...|\n",
      "|  9|2009-04-07|        mimismo|@twittera que me ...|\n",
      "| 10|2009-04-07| erinx3leannexo|spring break in p...|\n",
      "| 11|2009-04-07|   pardonlauren|I just re-pierced...|\n",
      "| 12|2009-04-07|           TLeC|@caregiving I cou...|\n",
      "| 13|2009-04-07|robrobbierobert|@octolinz16 It it...|\n",
      "| 14|2009-04-07|    bayofwolves|@smarrison i woul...|\n",
      "| 15|2009-04-07|     HairByJess|@iamjazzyfizzle I...|\n",
      "| 16|2009-04-07| lovesongwriter|Hollis' death sce...|\n",
      "| 17|2009-04-07|       armotley|about to file taxes |\n",
      "| 18|2009-04-07|     starkissed|@LettyA ahh ive a...|\n",
      "| 19|2009-04-07|      gi_gi_bee|@FakerPattyPattz ...|\n",
      "+---+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Read data from the MySQL table into a Spark DataFrame to Check if data was correctly created in mysql\n",
    "new_data_df = spark.read.jdbc(url=jdbc_url, table=\"tweets_mysql_processed\", properties=connection_properties)\n",
    "\n",
    "# Show the DataFrame\n",
    "new_data_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63c76188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been successfully exported to CSV.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the output path for the CSV file\n",
    "output_path = \"/home/hduser/Desktop/adv-data-big-data-ft-ca2-Paolacrir/tweets_mysql_processed.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "new_data_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"DataFrame has been successfully exported to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f7308ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame has been successfully exported to CSV: /home/hduser/Desktop/adv-data-big-data-ft-ca2-Paolacrir/tweets_mysql_processed.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert the Spark DataFrame to a pandas DataFrame\n",
    "pandas_df = new_data_df.toPandas()\n",
    "\n",
    "# Define the output path for the CSV file\n",
    "output_path = \"/home/hduser/Desktop/adv-data-big-data-ft-ca2-Paolacrir/tweets_mysql_processed.csv\"\n",
    "\n",
    "# Save the pandas DataFrame to a CSV file\n",
    "pandas_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"DataFrame has been successfully exported to CSV: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fef907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finish  Sparksession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a4e3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
