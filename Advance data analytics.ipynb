{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac7dbab4",
   "metadata": {},
   "source": [
    "# Master in Science in Data Analytics for Business\n",
    "## CA2 - Integrated CA2 \n",
    "\n",
    "- Student: Wendy Paola Espinoza Potoy\n",
    "- ID: 2021133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c34c694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc52d3df",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ProjectTweets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read data from file specifying header names\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tweets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProjectTweets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflag\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m tweets\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ProjectTweets.csv'"
     ]
    }
   ],
   "source": [
    "# Read data from file specifying header names\n",
    "tweets = pd.read_csv('ProjectTweets.csv', header=None, names=['ID', 'unknown', 'date', 'flag', 'user', 'text'])\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a89300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab613ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ee7b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to a datetime object without timezone information\n",
    "tweets['date'] = pd.to_datetime(tweets['date'].str.replace('PDT', ''), errors='coerce')\n",
    "\n",
    "# Display the DataFrame\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4d6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "tweets.drop(['ID', 'unknown', 'flag'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a37586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c564e1e",
   "metadata": {},
   "source": [
    "# Data Cleaning and text processing to apply sentimental analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a973c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a147d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    # Remove special characters and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) # Remove punctuation and numbers\n",
    "    text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = text.lower()   # Convert to lowercase\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7618143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Tokenize text\n",
    "    tokens = text.split()\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    # Join filtered tokens back into text\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aab0fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affe05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized(text):\n",
    "    \n",
    "    # Initialize the WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lemmatize each word and join back into a string\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d10a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatized(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "# Apply clean_text function to 'text' column\n",
    "tweets['clean_text'] = tweets['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1070214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea016fa4",
   "metadata": {},
   "source": [
    "## Sentimental Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fe33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob vaderSentiment nltk\n",
    "#pip install textblob nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44403af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate sentiment using TextBlob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Defining the function to analyze sentiment using TextBlob\n",
    "def textblob_sentiment(text):\n",
    "    # Perform sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment\n",
    "    \n",
    "    # Determine sentiment label\n",
    "    if sentiment.polarity > 0:\n",
    "        sentiment_label = 'positive'\n",
    "    elif sentiment.polarity < 0:\n",
    "        sentiment_label = 'negative'\n",
    "    else:\n",
    "        sentiment_label = 'neutral'\n",
    "    \n",
    "    # Return sentiment label and score\n",
    "    return sentiment_label, sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis to the tweets DataFrame\n",
    "tweets[['TextBlob sentiment', 'TextBlob score']] = tweets['clean_text'].apply(lambda text: pd.Series(textblob_sentiment(text)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b700e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the VADER lexicon \n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the VADER sentiment analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to analyze sentiment using VADER\n",
    "def analyze_sentiment(text):\n",
    "    # Perform sentiment analysis\n",
    "    scores = sid.polarity_scores(text)\n",
    "    \n",
    "    # Classify the sentiment\n",
    "    if scores['compound'] >= 0.05:\n",
    "        sentiment = 'positive'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        sentiment = 'negative'\n",
    "    else:\n",
    "        sentiment = 'neutral'\n",
    "    \n",
    "    # Return the sentiment label and scores\n",
    "    return sentiment, scores\n",
    "\n",
    "# Apply sentiment analysis to the tweets DataFrame\n",
    "tweets[['VADER sentiment', 'VADER compound', ]] = tweets['text'].apply(lambda text: pd.Series(analyze_sentiment(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb94366",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2446f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot TextBlob sentiment\n",
    "sns.countplot(data=tweets, x='TextBlob sentiment', order=['positive', 'neutral', 'negative'], ax=axes[0])\n",
    "axes[0].set_title('TextBlob Sentiment Analysis')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Plot VADER sentiment\n",
    "sns.countplot(data=tweets, x='VADER sentiment', order=['positive', 'neutral', 'negative'], ax=axes[1])\n",
    "axes[1].set_title('VADER Sentiment Analysis')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad65b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b61f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by date and sentiment\n",
    "tweets_sentiment_daily = tweets.groupby(tweets['date'].dt.date)['TextBlob sentiment'].value_counts().unstack(fill_value=0).reset_index()\n",
    "\n",
    "# Ensuring the 'date' column is in datetime format\n",
    "tweets_sentiment_daily['date'] = pd.to_datetime(tweets_sentiment_daily['date'])\n",
    "\n",
    "# Display the first 10 rows of the resulting DataFrame\n",
    "tweets_sentiment_daily.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5d289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_sentiment_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389be97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the values for each sentiment category\n",
    "total_sentiments = tweets_sentiment_daily[['negative', 'neutral', 'positive']].sum()\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot\n",
    "total_sentiments.plot(kind='bar', ax=ax, color=['red', 'gray', 'green'])\n",
    "\n",
    "# Setting labels and title\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Total Count')\n",
    "ax.set_title('Total Counts of Each Sentiment Category')\n",
    "ax.set_xticklabels(['Negative', 'Neutral', 'Positive'], rotation=0)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cae6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for each sentiment column\n",
    "tweets_sentiment_daily[['negative', 'neutral', 'positive']].hist(bins=50, figsize=(12, 6), layout=(1, 3))\n",
    "plt.suptitle('Distribution of Sentiment Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e8b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot box plots for each sentiment column\n",
    "tweets_sentiment_daily[['negative', 'neutral', 'positive']].plot(kind='box', subplots=True, layout=(1, 3), figsize=(12, 6))\n",
    "plt.suptitle('Box Plot of Sentiment Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Sentiment Analysis Over Time \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "width = 0.2  # Width of the bars\n",
    "dates = tweets_sentiment_daily['date']\n",
    "\n",
    "# Creating bar positions\n",
    "x = dates\n",
    "x1 = [d - pd.Timedelta(days=0.2) for d in dates]\n",
    "x2 = [d + pd.Timedelta(days=0.2) for d in dates]\n",
    "\n",
    "plt.bar(x1, tweets_sentiment_daily['negative'], width=width, label='Negative', color='red')\n",
    "plt.bar(x, tweets_sentiment_daily['neutral'], width=width, label='Neutral', color='blue')\n",
    "plt.bar(x2, tweets_sentiment_daily['positive'], width=width, label='Positive', color='green')\n",
    "\n",
    "plt.xticks(dates, rotation=45, fontsize=8)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Analysis Over Time')\n",
    "plt.grid(True, linewidth=0.2)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e32d2",
   "metadata": {},
   "source": [
    "note: There are gaps in dates to apply time series is required to fill missing dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate date range to fill missing days\n",
    "date_range = pd.date_range(start='2009-04-06', end='2009-06-25', freq='D')\n",
    "\n",
    "# Create DataFrame\n",
    "completed_dates = pd.DataFrame(date_range, columns=['date'])\n",
    "\n",
    "# Display the size and first few rows\n",
    "print('Size:', completed_dates.size)\n",
    "print(completed_dates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging actual data with complete dates\n",
    "dates_new = pd.merge(completed_dates, tweets_sentiment_daily,on='date', how='left')  \n",
    "dates_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d350b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Converting to DataFrame\n",
    "dates_new = pd.DataFrame(data)\n",
    "\n",
    "# Ensure 'date' column is in the DataFrame and convert to datetime\n",
    "if 'date' in dates_new.columns:\n",
    "    dates_new['date'] = pd.to_datetime(dates_new['date'])\n",
    "else:\n",
    "    print(\"Error: 'date' column not found in the DataFrame\")\n",
    "\n",
    "# Mean Imputation\n",
    "mean_imputed = dates_new.fillna(dates_new.mean(numeric_only=True))\n",
    "\n",
    "\n",
    "# K-Nearest Neighbors (KNN) Imputation\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "knn_imputed = dates_new.copy()\n",
    "knn_imputed[['negative', 'neutral', 'positive']] = imputer.fit_transform(dates_new[['negative', 'neutral', 'positive']])\n",
    "\n",
    "\n",
    "# Interpolation\n",
    "interp_imputed = dates_new.copy()\n",
    "interp_imputed = interp_imputed.interpolate(method='linear', limit_direction='forward')\n",
    "\n",
    "\n",
    "# Function to compare and determine the best method\n",
    "def compare_imputations(original, mean_imputed, knn_imputed, interp_imputed):\n",
    "\n",
    "  \n",
    "    # In this specific case, interpolation seems to follow the trend of the data better.\n",
    "    print(\"Conclusion: Interpolation seems to be the best method for this dataset\") \n",
    "# Compare the imputed DataFrames\n",
    "compare_imputations(dates_new, mean_imputed, knn_imputed, interp_imputed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d57ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA values with linear interpolation \n",
    "dates_new_int = dates_new.interpolate(method='linear', axis=0) \n",
    "\n",
    "# rounding to INT values\n",
    "for col in columns:\n",
    "    dates_new[col] = round(dates_new[col])\n",
    "\n",
    "dates_new_int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot before imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in columns:\n",
    "    plt.plot(dates_new['date'], dates_new[col], label=col)\n",
    "\n",
    "plt.title('Sentiment Counts Over Time (Before Imputation)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot after imputation\n",
    "plt.figure(figsize=(12, 6))\n",
    "for col in columns:\n",
    "    plt.plot(dates_new_int['date'], dates_new_int[col], label=col)\n",
    "\n",
    "plt.title('Sentiment Counts Over Time (After Imputation)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4349675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis Over Time  with imputations\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "width = 0.2  # Width of the bars\n",
    "dates = dates_new_int['date']\n",
    "\n",
    "# Creating bar positions\n",
    "x = dates\n",
    "x1 = [d - pd.Timedelta(days=0.2) for d in dates]\n",
    "x2 = [d + pd.Timedelta(days=0.2) for d in dates]\n",
    "\n",
    "plt.bar(x1, dates_new_int['negative'], width=width, label='Negative', color='red')\n",
    "plt.bar(x, dates_new_int['neutral'], width=width, label='Neutral', color='blue')\n",
    "plt.bar(x2, dates_new_int['positive'], width=width, label='Positive', color='green')\n",
    "\n",
    "plt.xticks(dates, rotation=45, fontsize=8)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment Analysis Over Time with imputations ')\n",
    "plt.grid(True, linewidth=0.2)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b00089",
   "metadata": {},
   "source": [
    "# Check time series data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd7d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dickey-Fuller Test for stationarity\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "for col in ['negative', 'neutral', 'positive']:\n",
    "    result = adfuller(dates_new_int[col])\n",
    "    print(f'ADF Statistic for {col}: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print('Stationary: Yes' if result[1] < 0.05 else 'Stationary: No')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8c1f54",
   "metadata": {},
   "source": [
    "Stationarity means that the statistical properties of a time series (such as mean, variance, and autocorrelation) do not change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cdbaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import numpy as np\n",
    "\n",
    "for col in ['negative', 'neutral', 'positive']:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Plot autocorrelation\n",
    "    plot_acf(dates_new_int[col], lags=30, ax=ax)\n",
    "    plt.title(f'Autocorrelation Plot for {col}')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    \n",
    "    # Annotate significant correlations\n",
    "    conf_level = 1.96 / np.sqrt(len(dates_new_int[col]))  # 95% confidence interval\n",
    "    ax.axhline(y=conf_level, linestyle='--', color='gray')  # Upper confidence interval\n",
    "    ax.axhline(y=-conf_level, linestyle='--', color='gray')  # Lower confidence interval\n",
    "    \n",
    "    # Perform ADF test for stationarity\n",
    "    result = adfuller(dates_new_int[col])\n",
    "    adf_stat = result[0]\n",
    "    p_value = result[1]\n",
    "    critical_values = result[4]\n",
    "    \n",
    "    # Determine if the series is stationary\n",
    "    is_stationary = p_value < 0.05\n",
    "    \n",
    "    # Add text annotation for ADF test result at the bottom of the plot\n",
    "    adf_text = f'ADF Statistic: {adf_stat:.2f}\\n' \\\n",
    "               f'p-value: {p_value:.2f}\\n' \\\n",
    "               f'Critical Values: {critical_values}\\n' \\\n",
    "               f'Stationary: {\"Yes\" if is_stationary else \"No\"}'\n",
    "    \n",
    "    ax.text(0.5, -0.25, adf_text, transform=ax.transAxes, fontsize=10, bbox=dict(facecolor='white', alpha=0.5),\n",
    "            verticalalignment='top', horizontalalignment='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seasonal Decomposition:\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Decompose the time series for each sentiment category\n",
    "for col in ['negative', 'neutral', 'positive']:\n",
    "    decomposition = seasonal_decompose(dates_new_int[col], model='additive', period=7)  # Assuming weekly seasonality\n",
    "    decomposition.plot()\n",
    "    plt.suptitle(f'Seasonal Decomposition of {col.capitalize()} Sentiment')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2bbb86",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6547dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime\n",
    "dates_new_int['date'] = pd.to_datetime(dates_new_int['date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52d5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the sentiment columns for modeling\n",
    "data = dates_new_int[['negative', 'neutral', 'positive']].values\n",
    "\n",
    "# Normalize the data to a small range \n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Split the data into training and testing sets (70% training, 30% testing)\n",
    "train_size = int(len(scaled_data) * 0.7)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a92dd9",
   "metadata": {},
   "source": [
    "# Applying Sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a16940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import itertools\n",
    "\n",
    "# Define the hyperparameters to test\n",
    "p = [1, 2]  # Autoregressive order\n",
    "d = [1]     # Differencing\n",
    "q = [1, 2]  # Moving average order\n",
    "P = [1]     # Seasonal autoregressive order\n",
    "D = [1]     # Seasonal differencing\n",
    "Q = [1]     # Seasonal moving average order\n",
    "s = [12]    # Seasonal period\n",
    "\n",
    "# Generate all possible combinations of hyperparameters\n",
    "hyperparameters_grid = list(itertools.product(p, d, q, P, D, Q, s))\n",
    "\n",
    "# Dictionary to store SARIMA models and their corresponding AIC values\n",
    "sarima_models_aic = {}\n",
    "\n",
    "# Loop through each combination of hyperparameters\n",
    "for hyperparameters in hyperparameters_grid:\n",
    "    # Fit SARIMA model with current hyperparameters to each sentiment category\n",
    "    for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "        model_sarima = SARIMAX(train_data[:, i], order=(hyperparameters[0], hyperparameters[1], hyperparameters[2]), \n",
    "                               seasonal_order=(hyperparameters[3], hyperparameters[4], hyperparameters[5], hyperparameters[6]), \n",
    "                               initialization='approximate_diffuse')\n",
    "        results_sarima = model_sarima.fit(enforce_invertibility=True)\n",
    "        sarima_models_aic[(sentiment, hyperparameters)] = results_sarima.aic\n",
    "\n",
    "# Find the hyperparameters with the lowest AIC for each sentiment category\n",
    "best_hyperparameters = {}\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    min_aic = min(sarima_models_aic[(sentiment, hyperparameters)] for hyperparameters in hyperparameters_grid)\n",
    "    best_hyperparameters[sentiment] = [hyperparameters for hyperparameters in hyperparameters_grid \n",
    "                                       if sarima_models_aic[(sentiment, hyperparameters)] == min_aic]\n",
    "\n",
    "# Print the best hyperparameters for each sentiment category\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    print(f\"Best hyperparameters for {sentiment}: {best_hyperparameters[sentiment]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d80a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#working with time series data adding sentiments and date and the scaling data\n",
    "\n",
    "columns = ['negative', 'neutral', 'positive']\n",
    "dates = pd.date_range(start='2009-04-06', periods=len(scaled_data), freq='D') \n",
    "data = pd.DataFrame(scaled_data, columns=columns, index=dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Best hyperparameters for each sentiment category\n",
    "best_hyperparameters = {\n",
    "    'negative': {'order': (2, 1, 1), 'seasonal_order': (1, 1, 1, 12)},\n",
    "    'neutral': {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 12)},\n",
    "    'positive': {'order': (1, 1, 1), 'seasonal_order': (1, 1, 1, 12)}\n",
    "}\n",
    "\n",
    "# Dictionary to store Sarima models\n",
    "sarima_models = {}\n",
    "\n",
    "# Fitting a SARIMA model to each sentiment category with best parameters\n",
    "for sentiment, params in best_hyperparameters.items():\n",
    "    model_sarima = SARIMAX(train_data[:, i], order=params['order'], seasonal_order=params['seasonal_order'],\n",
    "                           initialization='approximate_diffuse')\n",
    "    results_sarima = model_sarima.fit(enforce_invertibility=True)\n",
    "    sarima_models[sentiment] = results_sarima  # Store the fitted model\n",
    "    print(f\"Summary of SARIMA model for {sentiment}:\")\n",
    "    print(results_sarima.summary())\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62351fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate forecasts for each sentiment\n",
    "def forecast_sarima(models, steps):\n",
    "    forecasted = {}\n",
    "    for sentiment in models:\n",
    "        forecast = models[sentiment].get_forecast(steps=steps)\n",
    "        forecasted[sentiment] = forecast.predicted_mean\n",
    "    return pd.DataFrame(forecasted)\n",
    "\n",
    "# Forecast 1, 3, and 7 days ahead\n",
    "forecast_1day_sarima = forecast_sarima(sarima_models, 1)\n",
    "forecast_3days_sarima = forecast_sarima(sarima_models, 3)\n",
    "forecast_7days_sarima = forecast_sarima(sarima_models, 7)\n",
    "\n",
    "print(\"Forecast for 1 day ahead:\")\n",
    "print(forecast_1day_sarima)\n",
    "print(\"\\nForecast for 3 days ahead:\")\n",
    "print(forecast_3days_sarima)\n",
    "print(\"\\nForecast for 7 days ahead:\")\n",
    "print(forecast_7days_sarima)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203b991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine forecasts into DataFrame\n",
    "forecast_1day_array = forecast_1day_sarima.values\n",
    "forecast_3days_array = forecast_3days_sarima.values\n",
    "forecast_7days_array = forecast_7days_sarima.values\n",
    "\n",
    "\n",
    "# Inverse transform the forecasted values\n",
    "forecast_1day_sarima_rescaled = scaler.inverse_transform(forecast_1day_array)\n",
    "forecast_3days_sarima_rescaled = scaler.inverse_transform(forecast_3days_array)\n",
    "forecast_7days_sarima_rescaled = scaler.inverse_transform(forecast_7days_array)\n",
    "\n",
    "#printing results\n",
    "\n",
    "print(\"Forecast for 1 day ahead:\")\n",
    "print(forecast_1day_sarima_rescaled)\n",
    "print(\"\\nForecast for 3 days ahead:\")\n",
    "print(forecast_3days_sarima_rescaled)\n",
    "print(\"\\nForecast for 7 days ahead:\")\n",
    "print(forecast_7days_sarima_rescaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb136b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results in separate subplots\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 18), sharex=True)\n",
    "\n",
    "# Define color palette for each sentiment\n",
    "sentiment_colors = {\n",
    "    'negative': '#6baed6',  # Blue\n",
    "    'neutral': '#fd8d3c',   # Orange\n",
    "    'positive': '#31a354'   # Green\n",
    "}\n",
    "\n",
    "# Plot actual values\n",
    "for sentiment, color in sentiment_colors.items():\n",
    "    for ax in axs:\n",
    "        ax.plot(dates_new_int['date'], dates_new_int[sentiment], label=f'Actual ({sentiment})', color=color)\n",
    "\n",
    "# Prepare dates for forecast periods\n",
    "last_date = dates_new_int['date'].iloc[-1]\n",
    "forecast_dates_1day = [last_date + pd.DateOffset(days=1)]# Generate forecast dates for 1 days\n",
    "forecast_dates_3days = [last_date + pd.DateOffset(days=i) for i in range(1, 4)]# Generate forecast dates for 3 days\n",
    "forecast_dates_7days = [last_date + pd.DateOffset(days=i) for i in range(1, 8)]# Generate forecast dates for 7 days\n",
    "\n",
    "# Plot forecasted values\n",
    "for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "    axs[0].plot(forecast_dates_1day, forecast_1day_sarima_rescaled[:, i], marker='o', markersize=8, label=f'1 Day Forecast ({sentiment})', color=sentiment_colors[sentiment])\n",
    "    axs[1].plot(forecast_dates_3days, forecast_3days_sarima_rescaled[:, i], marker='o', markersize=8, label=f'3 Days Forecast ({sentiment})', color=sentiment_colors[sentiment])\n",
    "    axs[2].plot(forecast_dates_7days, forecast_7days_sarima_rescaled[:, i], marker='o', markersize=8, label=f'7 Days Forecast ({sentiment})', color=sentiment_colors[sentiment])\n",
    "\n",
    "# Set titles for subplots\n",
    "axs[0].set_title('1 Day Forecasting Results')\n",
    "axs[1].set_title('3 Days Forecasting Results')\n",
    "axs[2].set_title('7 Days Forecasting Results')\n",
    "\n",
    "# Set common labels\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7dcfc1",
   "metadata": {},
   "source": [
    "# Evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5acd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 5  # Adjust as needed\n",
    "\n",
    "# Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Initialize dictionaries to store performance metrics\n",
    "performance_metrics = {'MAE': {}, 'MSE': {}}\n",
    "\n",
    "# Perform cross-validation for each sentiment category\n",
    "for sentiment in best_hyperparameters:\n",
    "    # Initialize lists to store performance metrics for each fold\n",
    "    performance_metrics['MAE'][sentiment] = []\n",
    "    performance_metrics['MSE'][sentiment] = []\n",
    "    \n",
    "    # Extract sentiment data\n",
    "    data = train_data[:, i]  # Assuming train_data is the dataset\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for train_index, test_index in tscv.split(data):\n",
    "        # Split data into training and testing sets for the current fold\n",
    "        train_data_fold = data[train_index]\n",
    "        test_data_fold = data[test_index]\n",
    "        \n",
    "        # Fit SARIMA model to training data for the current fold\n",
    "        model_sarima = SARIMAX(train_data_fold, order=best_hyperparameters[sentiment]['order'], \n",
    "                               seasonal_order=best_hyperparameters[sentiment]['seasonal_order'],\n",
    "                               initialization='approximate_diffuse')\n",
    "        results_sarima = model_sarima.fit(enforce_invertibility=True)\n",
    "        \n",
    "        # Forecast sentiment values for the testing set\n",
    "        forecast = results_sarima.forecast(steps=len(test_data_fold))\n",
    "        \n",
    "        # Calculate forecast accuracy metrics for the current fold\n",
    "        mae = np.mean(np.abs(forecast - test_data_fold))\n",
    "        mse = np.mean((forecast - test_data_fold)**2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mape = np.mean(np.abs((test_data_fold - forecast) / test_data_fold)) * 100\n",
    "        \n",
    "        # Append performance metrics to lists\n",
    "        performance_metrics['MAE'][sentiment].append(mae)\n",
    "        performance_metrics['MSE'][sentiment].append(mse)\n",
    "       \n",
    "    \n",
    "\n",
    "# Compute average performance metrics across all folds\n",
    "avg_performance_metrics = {metric: {sentiment: np.mean(scores) for sentiment, scores in metrics.items()} \n",
    "                           for metric, metrics in performance_metrics.items()}\n",
    "\n",
    "# Print average performance metrics\n",
    "for metric, scores in avg_performance_metrics.items():\n",
    "    print(f\"Average {metric} Scores:\")\n",
    "    for sentiment, score in scores.items():\n",
    "        print(f\"{sentiment}: {score:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5e9ec",
   "metadata": {},
   "source": [
    "# Applying LTSM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363e7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0134 - accuracy: 0.4694\n",
      "Parameters: batch_size=128, epochs=150, learning_rate=0.001, num_lstm_units=128, Accuracy: 0.4693877696990967\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.3878\n",
      "Parameters: batch_size=128, epochs=150, learning_rate=0.0001, num_lstm_units=32, Accuracy: 0.3877550959587097\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0354 - accuracy: 0.3878\n",
      "Parameters: batch_size=128, epochs=150, learning_rate=0.0001, num_lstm_units=64, Accuracy: 0.3877550959587097\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0319 - accuracy: 0.3265\n",
      "Parameters: batch_size=128, epochs=150, learning_rate=0.0001, num_lstm_units=128, Accuracy: 0.3265306055545807\n",
      "Best Parameters: {'batch_size': 64, 'epochs': 100, 'learning_rate': 0.01, 'num_lstm_units': 128}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define function to create sequences and labels for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        labels.append(data[i+seq_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Define sequence length (number of time steps to look back)\n",
    "seq_length = 7  \n",
    "\n",
    "# Create sequences and labels for training\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "\n",
    "# Define a function to create the LSTM model\n",
    "def create_model(batch_size, epochs, learning_rate, num_lstm_units):\n",
    "    model = Sequential([\n",
    "        LSTM(num_lstm_units, input_shape=(seq_length, 3)),\n",
    "        Dense(3)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'num_lstm_units': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "for batch_size in param_grid['batch_size']:\n",
    "    for epochs in param_grid['epochs']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "            for num_lstm_units in param_grid['num_lstm_units']:\n",
    "                model = create_model(batch_size, epochs, learning_rate, num_lstm_units)\n",
    "                model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "                # Evaluate the model on validation set (if available)\n",
    "                # val_accuracy = model.evaluate(X_val, y_val)[1]  # Assuming you have validation data\n",
    "                # Uncomment the above line and replace X_val, y_val with your validation data\n",
    "                # if you have a validation set to evaluate the model performance\n",
    "                # For demonstration purpose, let's just use training accuracy as a proxy\n",
    "                val_accuracy = model.evaluate(X_train, y_train)[1]\n",
    "                print(f'Parameters: batch_size={batch_size}, epochs={epochs}, learning_rate={learning_rate}, num_lstm_units={num_lstm_units}, Accuracy: {val_accuracy}')\n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    best_params = {'batch_size': batch_size, 'epochs': epochs, 'learning_rate': learning_rate, 'num_lstm_units': num_lstm_units}\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c47e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create sequences and labels for LSTM\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "        labels.append(data[i+seq_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# Define sequence length (number of time steps to look back)\n",
    "seq_length = 7  # Example sequence length, tune as needed\n",
    "\n",
    "# Create sequences and labels for training\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "\n",
    "# Define the best parameters obtained from grid search\n",
    "best_params = {'batch_size': 64, 'epochs': 100, 'learning_rate': 0.01, 'num_lstm_units': 128}\n",
    "\n",
    "# Build an LSTM model with the best parameters\n",
    "model_lstm = Sequential([\n",
    "    LSTM(best_params['num_lstm_units'], input_shape=(seq_length, 3)),\n",
    "    Dense(3)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), loss='mse', metrics=['accuracy', \"mse\", \"mae\"])\n",
    "\n",
    "# Train the model\n",
    "history = model_lstm.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3904b27",
   "metadata": {},
   "source": [
    "# Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a9994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MAE and MSE values from history\n",
    "mae_values = history.history['mae']\n",
    "mse_values = history.history['mse']\n",
    "\n",
    "# Calculate average MAE and MSE\n",
    "avg_mae = np.mean(mae_values)\n",
    "avg_mse = np.mean(mse_values)\n",
    "\n",
    "# Find the epoch with the best accuracy\n",
    "best_epoch = np.argmax(history.history['accuracy']) + 1\n",
    "best_accuracy = history.history['accuracy'][best_epoch - 1]\n",
    "\n",
    "print(f'Average MAE: {avg_mae}')\n",
    "print(f'Average MSE: {avg_mse}')\n",
    "print(f'Best Accuracy: {best_accuracy} at epoch {best_epoch}' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9126a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss Over Epochs')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy Over Epochs')\n",
    "ax2.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e847c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forecasts for 1 day, 3 days, and 7 days\n",
    "forecast_1day = model_lstm.predict(np.expand_dims(X_train[-1], axis=0))\n",
    "forecast_3days = model_lstm.predict(X_train[-3:])\n",
    "forecast_7days = model_lstm.predict(X_train[-7:])\n",
    "\n",
    "# Print forecasts\n",
    "print(\"Forecast for 1 day ahead:\")\n",
    "print(forecast_1day)\n",
    "print(\"\\nForecast for 3 days ahead:\")\n",
    "print(forecast_3days)\n",
    "print(\"\\nForecast for 7 days ahead:\")\n",
    "print(forecast_7days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0db8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to forecast future values\n",
    "def forecast(model, data, seq_length, days_ahead):\n",
    "    forecasted = []\n",
    "    current_seq = data[-seq_length:]  # Start with the last available sequence\n",
    "    for _ in range(days_ahead):\n",
    "        pred = model.predict(current_seq[np.newaxis, :, :]).flatten()\n",
    "        forecasted.append(pred)\n",
    "        current_seq = np.append(current_seq[1:], [pred], axis=0)\n",
    "    return np.array(forecasted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ecb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescalind the forecasted values back to original scale\n",
    "forecast_1day_rescaled = scaler.inverse_transform(forecast_1day)\n",
    "forecast_3days_rescaled = scaler.inverse_transform(forecast_3days)\n",
    "forecast_7days_rescaled = scaler.inverse_transform(forecast_7days)\n",
    "print(\"Forecast for 1 day ahead:\")\n",
    "print(forecast_1day_rescaled)\n",
    "print(\"\\nForecast for 3 days ahead:\")\n",
    "print(forecast_3days_rescaled)\n",
    "print(\"\\nForecast for 7 days ahead:\")\n",
    "print(forecast_7days_rescaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18b43f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot actual values\n",
    "plt.plot(dates_new_int['date'], dates_new_int[['negative', 'neutral', 'positive']], label='Actual')\n",
    "\n",
    "# Prepare dates for forecast periods\n",
    "last_date = dates_new_int['date'].iloc[-1]\n",
    "forecast_dates_1day = [last_date + pd.DateOffset(days=1)]\n",
    "forecast_dates_3days = [last_date + pd.DateOffset(days=i) for i in range(1, 4)]\n",
    "forecast_dates_7days = [last_date + pd.DateOffset(days=i) for i in range(1, 8)]\n",
    "\n",
    "# Plot forecasted values\n",
    "for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "    plt.plot(forecast_dates_1day, forecast_1day_rescaled[:, i], marker='o', markersize=8, label=f'1 Day Forecast ({sentiment})')\n",
    "    plt.plot(forecast_dates_3days, forecast_3days_rescaled[:, i], marker='o', markersize=8, label=f'3 Days Forecast ({sentiment})')\n",
    "    plt.plot(forecast_dates_7days, forecast_7days_rescaled[:, i], marker='o', markersize=8, label=f'7 Days Forecast ({sentiment})')\n",
    "\n",
    "plt.title('Forecasting Results')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff945e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results in separate subplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(12, 18), sharex=True)\n",
    "\n",
    "# Plot actual values\n",
    "# Define color palette for each sentiment\n",
    "sentiment_colors = {\n",
    "    'negative': '#6baed6',  # Blue\n",
    "    'neutral': '#fd8d3c',   # Orange\n",
    "    'positive': '#31a354'   # Green\n",
    "}\n",
    "\n",
    "# Plot actual values\n",
    "for sentiment, color in sentiment_colors.items():\n",
    "    for ax in axs:\n",
    "        ax.plot(dates_new_int['date'], dates_new_int[sentiment], label=f'Actual ({sentiment})', color=color)\n",
    "\n",
    "# Prepare dates for forecast periods\n",
    "last_date = dates_new_int['date'].iloc[-1]\n",
    "forecast_dates_1day = [last_date + pd.DateOffset(days=1)]# Generate forecast dates for 1 days\n",
    "forecast_dates_3days = [last_date + pd.DateOffset(days=i) for i in range(1, 4)]# Generate forecast dates for 3 days\n",
    "forecast_dates_7days = [last_date + pd.DateOffset(days=i) for i in range(1, 8)]# Generate forecast dates for 7 days\n",
    "\n",
    "# Plot forecasted values for 1 day ahead\n",
    "for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "    axs[0].plot(forecast_dates_1day, forecast_1day_rescaled[:, i], marker='o', markersize=8, label=f'1 Day Forecast ({sentiment})', color=sentiment_colors[sentiment])\n",
    "\n",
    "# Plot forecasted values for 3 days ahead\n",
    "for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "    axs[1].plot(forecast_dates_3days, forecast_3days_rescaled[:, i], marker='o', markersize=8, label=f'3 Days Forecast ({sentiment})', color=sentiment_colors[sentiment])\n",
    "\n",
    "# Plot forecasted values for 7 days ahead\n",
    "for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "    axs[2].plot(forecast_dates_7days, forecast_7days_rescaled[:, i], marker='o', markersize=8, label=f'7 Days Forecast ({sentiment})', color=sentiment_colors[sentiment])\n",
    "\n",
    "# Setting titles and labels for subplots\n",
    "axs[0].set_title('1 Day Forecasting Results')\n",
    "axs[1].set_title('3 Days Forecasting Results')\n",
    "axs[2].set_title('7 Days Forecasting Results')\n",
    "\n",
    "# Setting x-axis limits to include the forecast period\n",
    "axs[0].set_xlim(dates_new_int['date'].iloc[0], forecast_dates_1day[-1])\n",
    "axs[1].set_xlim(dates_new_int['date'].iloc[0], forecast_dates_3days[-1])\n",
    "axs[2].set_xlim(dates_new_int['date'].iloc[0], forecast_dates_7days[-1])\n",
    "\n",
    "# Setting common labels\n",
    "for ax in axs:\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.label_outer()  # Hide x labels and tick labels for top plots and y ticks for right plots.\n",
    "\n",
    "axs[2].set_xlabel('Date')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb3805b",
   "metadata": {},
   "source": [
    "# Dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_html_components as html\n",
    "import dash_core_components as dcc\n",
    "import plotly.express as px\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1dabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets\n",
    "# Count the occurrences of each TextBlob sentiment\n",
    "blob_sentiment_counts = df['TextBlob sentiment'].value_counts().reset_index()\n",
    "blob_sentiment_counts.columns = ['TextBlob sentiment', 'count']\n",
    "\n",
    "\n",
    "vader_sentiment_counts = df['VADER sentiment'].value_counts().reset_index()\n",
    "vader_sentiment_counts.columns = ['VADER sentiment', 'count']\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_sentiment_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_sentiment_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b2e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dates_new\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c7c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dates_new_int\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64814395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to long format\n",
    "df1_melted = df1.melt(id_vars=[\"date\"], var_name=\"sentiment\", value_name=\"count\")\n",
    "df1_melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to long format\n",
    "df2_melted = df2.melt(id_vars=[\"date\"], var_name=\"sentiment\", value_name=\"count\")\n",
    "df2_melted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfcebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dash application\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define color mapping for sentiments\n",
    "sentiment_colors = {\"negative\": \"#EF553B\", \"neutral\": \"#636EFA\", \"positive\": \"#00CC96\"}\n",
    "\n",
    "\n",
    "# Define tab content\n",
    "tab1_content = html.Div([\n",
    "    html.H2(\"LSTM Model\"),\n",
    "    dcc.Dropdown(\n",
    "        id='dropdown-tab1',\n",
    "        options=[\n",
    "            {'label': '1 Day Forecasting', 'value': 'opt1'},\n",
    "            {'label': '3 Days Forecasting', 'value': 'opt2'},\n",
    "            {'label': '7 Days Forecasting', 'value': 'opt3'}\n",
    "        ],\n",
    "        value='opt1'\n",
    "    ),\n",
    "    html.Div(id='output-tab1')\n",
    "])\n",
    "\n",
    "tab2_content = html.Div([\n",
    "    html.H2(\"Sarima Model\"),\n",
    "    dcc.Dropdown(\n",
    "        id='dropdown-tab2',\n",
    "        options=[\n",
    "           {'label': '1 Day Forecasting', 'value': 'opt4'},\n",
    "           {'label': '3 Days Forecasting', 'value': 'opt5'},\n",
    "           {'label': '7 Days Forecasting', 'value': 'opt6'}\n",
    "        ],\n",
    "        value='opt4'\n",
    "    ),\n",
    "    html.Div(id='output-tab2')\n",
    "])\n",
    "\n",
    "\n",
    "# Function to create forecast subplots\n",
    "def create_forecast_subplot(forecast_dates, forecast_data, title):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot original data\n",
    "    for sentiment in ['negative', 'neutral', 'positive']:\n",
    "        fig.add_trace(go.Scatter(x=df2_melted[\"date\"], \n",
    "                                 y=df2_melted[df2_melted['sentiment'] == sentiment][\"count\"],\n",
    "                                 mode='lines', \n",
    "                                 name=f'Original Data ({sentiment})',\n",
    "                                 line=dict(color=sentiment_colors[sentiment])))\n",
    "    \n",
    "    # Plot forecasted data\n",
    "    for i, sentiment in enumerate(['negative', 'neutral', 'positive']):\n",
    "        fig.add_trace(go.Scatter(x=forecast_dates, \n",
    "                                 y=forecast_data[:, i], \n",
    "                                 mode='markers+lines', \n",
    "                                 name=f'{title} Forecast ({sentiment})',\n",
    "                                 line=dict(color=sentiment_colors[sentiment]),\n",
    "                                 marker=dict(color=sentiment_colors[sentiment])))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(title=title,\n",
    "                      xaxis_title=\"Date\",\n",
    "                      yaxis_title=\"Count\",\n",
    "                      legend_title=\"Sentiment\",\n",
    "                      template=\"plotly_white\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######Page1######## Graphs\n",
    "# Create a Plotly bar graph for TextBlob\n",
    "fig_blob = px.bar(\n",
    "    blob_sentiment_counts,\n",
    "    x='TextBlob sentiment',\n",
    "    y='count',\n",
    "    title='TextBlob Sentiment Counts',\n",
    "    color='TextBlob sentiment',\n",
    "    color_discrete_map=sentiment_colors\n",
    ")\n",
    "\n",
    "# Create a Plotly bar graph for Vader\n",
    "fig_vader = px.bar(\n",
    "    vader_sentiment_counts,\n",
    "    x='VADER sentiment',\n",
    "    y='count',\n",
    "    title='Vader Sentiment Counts',\n",
    "    color='VADER sentiment',\n",
    "    color_discrete_map=sentiment_colors\n",
    ")\n",
    "\n",
    "#####################\n",
    "#######Page2######## Graphs\n",
    "# Create a Plotly bar graph for TextBlob\n",
    "\n",
    "fig_counts_over_time = px.line(df1_melted, x=\"date\", y=\"count\", color=\"sentiment\", title=\"Sentiment Analysis over Time Before Imputation\",\n",
    "              labels={\"date\": \"Date\", \"count\": \"Count\", \"sentiment\": \"Sentiment\"},\n",
    "              color_discrete_map=sentiment_colors)\n",
    "\n",
    "\n",
    "fig_counts_over_time_with_imputation = px.line(df2_melted, x=\"date\", y=\"count\", color=\"sentiment\", title=\"Sentiment Analysis over Time After Imputation\",\n",
    "              labels={\"date\": \"Date\", \"count\": \"Count\", \"sentiment\": \"Sentiment\"},\n",
    "              color_discrete_map=sentiment_colors)\n",
    "\n",
    "#####################\n",
    "#######Page3######## Graphs\n",
    "# Create forecast graphs for each period for LSTM\n",
    "fig_forecast_1day = create_forecast_subplot(forecast_dates_1day, forecast_1day_rescaled, \"1 Day\")\n",
    "fig_forecast_3days = create_forecast_subplot(forecast_dates_3days, forecast_3days_rescaled, \"3 Days\")\n",
    "fig_forecast_7days = create_forecast_subplot(forecast_dates_7days, forecast_7days_rescaled, \"7 Days\")\n",
    "\n",
    "\n",
    "# Create forecast graphs for each period for SARIMA\n",
    "fig_forecast_1day_sarima = create_forecast_subplot(forecast_dates_1day, forecast_1day_sarima_rescaled , \"1 Day\")\n",
    "fig_forecast_3days_sarima = create_forecast_subplot(forecast_dates_3days, forecast_3days_sarima_rescaled , \"3 Days\")\n",
    "fig_forecast_7days_sarima = create_forecast_subplot(forecast_dates_7days, forecast_7days_sarima_rescaled , \"7 Days\")\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Create a Dash application with Bootstrap\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP], suppress_callback_exceptions=True)\n",
    "\n",
    "\n",
    "# Define the sidebar layout\n",
    "sidebar = html.Div(\n",
    "    [\n",
    "        dbc.NavLink(html.Img(src='https://www.cct.ie/wp-content/uploads/CCT_Logo_New_Aug_17-578x200.png', style={'width': '230px', 'height': 'auto', 'margin': '0 0 0 -15px'}), href=\"/sentiment-analysis\", id=\"home-link\", className=\"nav-link\"),  # Link to sentiment analysis page\n",
    "        html.Hr(),\n",
    "        dbc.Nav(\n",
    "            [\n",
    "                dbc.NavLink(\"Sentiment Analysis\", href=\"/sentiment-analysis\", id=\"page-1-link\", active=\"exact\"),\n",
    "                dbc.NavLink(\"Sentiment Time Series\", href=\"/time-series\", id=\"page-2-link\", active=\"exact\"),\n",
    "                dbc.NavLink(\"Forecast Models\", href=\"/forecast-models\", id=\"page-3-link\", active=\"exact\"),\n",
    "            ],\n",
    "            vertical=True,\n",
    "            pills=True,\n",
    "        ),\n",
    "        html.Footer(\n",
    "            [\n",
    "                html.P(\"Master in Data Analytics - CA2 2024\", style={\"text-align\": \"center\", \"margin-bottom\": \"10px\"}),\n",
    "                html.P(\"Wendy Paola Espinoza Potoy\", style={\"text-align\": \"center\", \"font-size\": \"10px\", \"margin\": \"0\"}),\n",
    "                html.P(\"Student ID: 2021133\", style={\"text-align\": \"center\", \"font-size\": \"10px\", \"margin\": \"0\"})\n",
    "            ],\n",
    "            style={\"background-color\": \"#f8f9fa\", \"padding\": \"10px\", \"margin-top\": \"auto\"}\n",
    "        )\n",
    "    ],\n",
    "    style={\"display\": \"flex\", \"flex-direction\": \"column\", \"min-height\": \"100vh\", \"position\": \"fixed\", \"top\": 0, \"left\": 0, \"bottom\": 0, \"width\": \"14rem\", \"padding\": \"0.5rem 0.5rem\", 'background-color': '#f8f9fa', 'box-shadow': '2px 0 5px rgba(0, 0, 0, 0.1)'},\n",
    ")\n",
    "\n",
    "# Define the content layout\n",
    "content = html.Div(id=\"page-content\", style={\"margin-left\": \"16rem\", \"padding\": \"1rem\"})\n",
    "\n",
    "# Define the callback to update the page content based on the URL\n",
    "@app.callback(\n",
    "    Output(\"page-content\", \"children\"),\n",
    "    [Input(\"url\", \"pathname\")]\n",
    ")\n",
    "def render_page_content(pathname):\n",
    "    if pathname == \"/sentiment-analysis\":\n",
    "        return [\n",
    "            html.H2(\"Sentiment Analysis\"),\n",
    "            html.Div(\"Visualization of sentiment values in the dataset.\"),\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dcc.Graph(id='sentiment-bar-chart-1', figure=fig_blob), width=6),\n",
    "                    dbc.Col(dcc.Graph(id='sentiment-bar-chart-2', figure=fig_vader), width=6)\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    elif pathname == \"/time-series\":\n",
    "        return [\n",
    "            html.H2(\"Sentiment Count over time Analysis\"),\n",
    "            html.Div(\"Visualization of sentiment values in the dataset.\"),\n",
    "            dbc.Col(dcc.Graph(id='sentiment-bar-chart-1', figure=fig_counts_over_time)),\n",
    "            dbc.Col(dcc.Graph(id='sentiment-bar-chart-2', figure=fig_counts_over_time_with_imputation)),\n",
    "        ]\n",
    "    elif pathname == \"/forecast-models\":\n",
    "        return [ \n",
    "            dcc.Tabs([\n",
    "                dcc.Tab(label='LSTM Model', children=tab1_content),\n",
    "                dcc.Tab(label='Sarima Model', children=tab2_content),\n",
    "            ])\n",
    "        ]\n",
    "\n",
    "    # If the URL is invalid, return a 404 error\n",
    "    return html.H1(\"404 - Not Found\")\n",
    "\n",
    "\n",
    "\n",
    "# Define callbacks\n",
    "@app.callback(\n",
    "    Output('output-tab1', 'children'),\n",
    "    [Input('dropdown-tab1', 'value')]\n",
    ")\n",
    "\n",
    "def update_output_tab1(selected_value):\n",
    "    if selected_value == 'opt1':\n",
    "        return [\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dcc.Graph(id='1day_forecast', figure=fig_forecast_1day))\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    elif selected_value == 'opt2':\n",
    "        return [\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dcc.Graph(id='3day_forecast', figure=fig_forecast_3days))\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    elif selected_value == 'opt3':\n",
    "        return [\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dcc.Graph(id='7days_forecast', figure=fig_forecast_7days))\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "@app.callback(\n",
    "    Output('output-tab2', 'children'),\n",
    "    [Input('dropdown-tab2', 'value')]\n",
    ")\n",
    "def update_output_tab2(selected_value):\n",
    "    if selected_value == 'opt4':\n",
    "        return [\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dcc.Graph(id='1day_forecast_sarima', figure=fig_forecast_1day_sarima))\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    elif selected_value == 'opt5':\n",
    "        return [\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dcc.Graph(id='1day_forecast', figure=fig_forecast_3days_sarima))\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    elif selected_value == 'opt6':\n",
    "        return [\n",
    "            dbc.Row(\n",
    "                [\n",
    "                    dbc.Col(dcc.Graph(id='1day_forecast', figure=fig_forecast_7days_sarima))\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "# Define the layout of the app\n",
    "app.layout = html.Div([\n",
    "    dcc.Location(id=\"url\", refresh=False, pathname=\"/sentiment-analysis\"),  # Set default URL\n",
    "    sidebar,\n",
    "    content\n",
    "], id=\"wrapper\")\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    #app.run(debug=False, host='127.0.0.1', port=8052)\n",
    "    app.run_server(debug=True, host='127.0.0.1', port=8051)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
